{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMgzF2EZyrYlRDZ5U2CtY14",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tyobeka/recipe-generator/blob/main/training_an_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Purpose of this Notebook\n",
        "\n",
        "The purpose of this notebook is to provide a template for preparing data and training an RNN for sequence-to-sequence modelling. A lot of the code is based on lab exercise that I completed for the [2025 MIT Introduction to Deep Learning course](https://github.com/MITDeepLearning/introtodeeplearning/tree/master) that I have been working through independently."
      ],
      "metadata": {
        "id": "RMBU8A6oJ0MF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installing Dependencies.\n",
        "\n",
        "Before we can start working, we need to install dependencies, and import relevant packages needed for this task."
      ],
      "metadata": {
        "id": "VtpiJGWAKTRT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh5bXV2bJWYH"
      },
      "outputs": [],
      "source": [
        "# comet\n",
        "!pip install comet_ml > /dev/null 2>&1\n",
        "import comet_ml\n",
        "COMET_API_KEY = \"XYdAKTEOdcNCOQmuvd2YICcmp\"\n",
        "\n",
        "assert COMET_API_KEY != \"\", \"Please insert your Comet API Key\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch and relevant libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm import tqdm\n",
        "\n",
        "assert torch.cuda.is_available(), \"Please enable GPU from runtime settings\""
      ],
      "metadata": {
        "id": "dOSXE0AoLXhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MIT Introduction to Deep Learning package and relevant packages\n",
        "# for music generation task\n",
        "!pip install mitdeeplearning --quiet\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "from scipy.io.wavfile import write\n",
        "!apt-get install abcmidi timidity > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "LxTf4GxoLlot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Loading Dataset\n",
        "\n",
        "The dataset used for the music generation task consists of Irish folk songs, represented in ABC notation. Along with each song, is its meta data containing additional information about the song:\n",
        "- X: the songs index\n",
        "- T: title of the song\n",
        "- Z: unique identifier of the song\n",
        "- M: a feature of the song?\n",
        "- L: the tempo of the song?\n",
        "- K: the key that the song is played in?"
      ],
      "metadata": {
        "id": "fiwyOvBgMJGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "songs = mdl.lab1.load_training_data()\n",
        "\n",
        "# print an example of a song\n",
        "example_song = songs[0]\n",
        "print(\"\\nExample song: \")\n",
        "print(example_song)"
      ],
      "metadata": {
        "id": "am7fHB9tMugj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the following code to convert the ABC notation to an audio file to listen to\n",
        "# mdl.lab1.play_song(example_song)"
      ],
      "metadata": {
        "id": "SomcBM7pM__n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Tokenizing the Dataset\n",
        "\n",
        "This involves determining what the basic unit of each observation in the dataset is, and generating a vocabulary that consists of a collection of these units that can be sequenced together to represent each observation in the dataset."
      ],
      "metadata": {
        "id": "TLIuYAG0NKth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# join list of songs into single string containing all songs\n",
        "songs_joined = \"\\n\\n\".join(songs)\n",
        "\n",
        "# find all unique characters in the joined string\n",
        "vocab = sorted(set(songs_joined))\n",
        "print(\"There are\", len(vocab), \"unique characters in the dataset\")"
      ],
      "metadata": {
        "id": "hsiEC5ckOpa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocessing the Dataset\n",
        "\n",
        "Having decided what the basic unit of an observation, we can represent the data in the correct format to be processed by the RNN model."
      ],
      "metadata": {
        "id": "cyRO-Bj7PGvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorizing the text\n",
        "\n",
        "In order for the model to process the data, we need to create a numerical representation for the text-based data. To do this, two lookup tables are generated:\n",
        "\n",
        "1. `char2idx`: Maps characters to numbers.\n",
        "2. `idx2char`: Maps numbers back to characters."
      ],
      "metadata": {
        "id": "0UD1MkTzPqyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# print an example to show how the tables work\n",
        "print(f\"The index for 'm': {char2idx['m']}.\")\n",
        "print(f\"The character corresponding to index 68: '{idx2char[68]}'.\")"
      ],
      "metadata": {
        "id": "sebCbScBQUMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the songs in the `songs_joined` string:"
      ],
      "metadata": {
        "id": "un2OFikNRodM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_string(string):\n",
        "  \"\"\"\n",
        "  A function to convert all songs to a numeric representation.\n",
        "\n",
        "  Output: an np.array of dimension N elements, were N is the number of characters\n",
        "  in the input string.\n",
        "  \"\"\"\n",
        "  output = []\n",
        "  for char in string:\n",
        "    output.append(char2idx[char])\n",
        "\n",
        "  return np.array(output)\n",
        "\n",
        "vectorized_songs = vectorize_string(songs_joined)"
      ],
      "metadata": {
        "id": "Gb2kx8elRjJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating training inputs and targets\n",
        "\n",
        "The input to an RNN is a sequence of characters of length `seq_length`. For this task, we will also define a target sequence of the same length, except that that it will be shifted one character to the right.\n",
        "\n",
        "For example if the `seq_length` is 4 and the text is \"Hello\", then the input sequence will be \"Hell\" and the target sequence will be \"ello\". Meaning that the text in the data needs to broken into chunks of `seq_length + 1` for training.\n",
        "\n",
        "It also means that after training, we should be able to generate a single a character, or short sequence of characters.\n",
        "\n",
        "We will also be batching data for training, so a batch function will be written to do so:"
      ],
      "metadata": {
        "id": "QbD-9oIGSnzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(vectorized_songs, seq_length, batch_size):\n",
        "\n",
        "  # highest index in vectorized_songs: 0 to n\n",
        "  n = vectorized_songs.shape[0] - 1\n",
        "\n",
        "  # randomly choose the starting indices of input examples in the training batch\n",
        "  idx = np.random.choice(n - seq_length, batch_size)\n",
        "\n",
        "  input_batch = []\n",
        "  output_batch = []\n",
        "  for i in idx:\n",
        "    input_batch.append(vectorized_songs[i:i+seq_length])\n",
        "    output_batch.append(vectorized_songs[i+1:i+seq_length+1])\n",
        "\n",
        "  # covert batches to tensors\n",
        "  x_batch = torch.tensor(input_batch, dtype=torch.long)\n",
        "  y_batch = torch.tensor(output_batch, dtype=torch.long)\n",
        "\n",
        "  return x_batch, y_batch\n",
        "\n",
        "x_batch, y_batch = get_batch(vectorized_songs, seq_length=10, batch_size=2)\n",
        "print(f\"x_batch shape: {x_batch.shape}\")\n",
        "print(f\"y_batch shape: {y_batch.shape}\")"
      ],
      "metadata": {
        "id": "ZQoX8AwQUabC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some code to illustrate why we define the target sequence as a sequence of the same length as its corresponding input sequence, but shifted one character to the right:"
      ],
      "metadata": {
        "id": "2cxFHUIHWUEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(x_batch[0], y_batch[0])):\n",
        "    print(\"Step {:3d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx.item()])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx.item()])))"
      ],
      "metadata": {
        "id": "VYrraflgWx7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For each input character, we output a character that is then fed back into the model as input: this will enable the model to generate a song, one character at a time.**"
      ],
      "metadata": {
        "id": "QyH9WKv9XEvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Define the Recurrent Neural Network (RNN) Model\n",
        "\n",
        "The model that we will be using is based off the **LSTM architecture**, that uses two state vectors to maintain information about the temporal relationships between the consecutive characters:\n",
        "- Cell state:\n",
        "\n",
        "$$ \\mathbf{C}^*_t= tanh\\left(\\mathbf{W}_C\\left[\\mathbf{h}_{t-1}, \\mathbf{x}_t\\right] + \\mathbf{b}_C\\right)$$\n",
        "\n",
        "$$ C_t = \\mathbf{f}_t*\\mathbf{C}^*_{t-1} + \\mathbf{i}_t*\\mathbf{C}^*_t$$\n",
        "\n",
        "- Hidden state:\n",
        "$$ \\mathbf{o}_t = \\sigma \\left(\\mathbf{W}_o\\left[\\mathbf{h}_{t-1}, \\mathbf{x}_t\\right] + \\mathbf{b}_o\\right)$$\n",
        "$$ \\mathbf{h}_t = \\mathbf{o}_t * tanh(\\mathbf{C}_t) $$\n",
        "\n",
        "where,\n",
        "- $\\mathbf{i}_t = \\sigma \\left(\\mathbf{W}_i\\left[\\mathbf{h}_{t-1}, \\mathbf{x}_t\\right] + \\mathbf{b}_i\\right)$ is an **\"input gate layer\"** gate that decides which values in the cell state to update.\n",
        "- $\\mathbf{f}_t = \\sigma \\left(\\mathbf{W}_i\\left[\\mathbf{h}_{t-1}, \\mathbf{x}_t\\right] + \\mathbf{f}_i\\right)$ is a **\"forget gate layer\"** gate that decides which layers from the previous cell state to get rid of.\n",
        "- $\\mathbf{o}_t$ is **\"output layer\"** gate that decides which parts of the cell state we're going to output as the hidden state.\n",
        "\n",
        "See the following [link](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) for more details."
      ],
      "metadata": {
        "id": "_AqRgcS0XdOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the PyTorch's `nn.Module` to define the RNN:"
      ],
      "metadata": {
        "id": "dIvZOTyLXreH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(LSTMModel, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    # define different components of lstm\n",
        "    self.embedding = nn.Embedding(\n",
        "        num_embeddings=vocab_size,\n",
        "        embedding_dim=embedding_dim\n",
        "        )\n",
        "\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size=embedding_dim,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        dropout=0,\n",
        "        bidirectional=False\n",
        "        )\n",
        "    self.fc = nn.Linear(\n",
        "        in_features=hidden_size,\n",
        "        out_features=vocab_size\n",
        "    )\n",
        "\n",
        "    # for t=0, C_t and h_t are zero vectors\n",
        "    def init_hidden(self, batch_size, device):\n",
        "      return (torch.zeros(1, batch_size, self.hidden_size).to(device),\n",
        "              torch.zeros(1, batch_size, self.hidden_size).to(device))\n",
        "\n",
        "    # forward function\n",
        "    def forward(self, x, state=None, return_state=False):\n",
        "      x = self.embedding(x)\n",
        "\n",
        "      if state is None:\n",
        "        state = self.init_hidden(x.size(0), x.device)\n",
        "\n",
        "      out, state = self.lstm(x, state)\n",
        "      out = self.fc(out)\n",
        "\n",
        "      if return_state:\n",
        "        return (out, state)\n",
        "      else:\n",
        "        return out"
      ],
      "metadata": {
        "id": "P6nCrqL2o6Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate the model to see what it looks like:"
      ],
      "metadata": {
        "id": "F9FFpH6prgdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define parameters\n",
        "params = dict(\n",
        "  batch_size = 8,\n",
        "  embedding_dim = 256,\n",
        "  hidden_size = 1024,\n",
        "  vocab_size = len(vocab),\n",
        "  seq_length = 100,\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  )\n",
        "\n",
        "# instantiate model\n",
        "model = LSTMModel(\n",
        "    vocab_size=params['vocab_size'],\n",
        "    embedding_dim=params['embedding_dim'],\n",
        "    hidden_size=params['hidden_size']\n",
        "    )\n",
        "\n",
        "# move it to the correct device\n",
        "model = model.to(params['device'])\n",
        "\n",
        "# print the model\n",
        "print(model)"
      ],
      "metadata": {
        "id": "psrfv-birf8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us test the model to check whether it performs as expected:"
      ],
      "metadata": {
        "id": "92avyvc2slKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = get_batch(vectorized_songs=vectorized_songs, seq_length=params['seq_length'], batch_size=params['batch_size'])\n",
        "x = x.to(params['device'])\n",
        "y = y.to(params['device'])\n",
        "\n",
        "yhat = model(x)\n",
        "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
        "print(\"Model output shape: \", yhat.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "id": "lwRRvUoXsq5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing predictions\n",
        "\n",
        "To get actual predictions from the model, we need to begin by defining an output distribution: `torch.softmax` is applied over the output logits. The output distribution is a categorical distribution, we then sample from this distribution to obtain a prediction using `torch.multinomial`.\n",
        "\n",
        "**Note:** we sample from the output distribution, as opposed to simply taking `argmax` to avoid the model getting stuck in a repetitive loop, outputting the same character multiple times in the output."
      ],
      "metadata": {
        "id": "hjmbfWyk4V24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing a prediction\n",
        "example_logit = yhat[0]\n",
        "sampled_indices = torch.multinomial(torch.softmax(example_logit, dim=-1), num_samples = 1)\n",
        "sampled_indices = sampled_indices.squeeze(-1).cpu().numpy()\n",
        "sampled_indices"
      ],
      "metadata": {
        "id": "rla1328G6NiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoding the index to see the text produced\n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0].cpu()])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
      ],
      "metadata": {
        "id": "qCuCof5L6uNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}